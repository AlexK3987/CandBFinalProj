{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from kmodes.kmodes import KModes\n",
    "import csv\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "# codes.csv : ID,Parameter_ID,Name,Description,Number\n",
    "# languages.csv :ID,Name,Macroarea,Latitude,Longitude,Glottocode,ISO639P3code\n",
    "# parameters.csv : ID,Name,Description\n",
    "# values.csv : ID,Language_ID,Parameter_ID,Value,Code_ID,Comment,Source,Contribution_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to determine if category is binary, categorical, or continuous`\n",
    "# need a map from (param_id,name) -> number\n",
    "param_info = dict()\n",
    "file_path = './wals_dataset.cldf/'\n",
    "with open(file_path + 'codes.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        uuid, param_id, name, desc, num = row\n",
    "        if param_id not in param_info.keys():\n",
    "            param_info[param_id] = dict()\n",
    "        param_info[param_id][name] = int(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Determine categorical variables\n",
    "\n",
    "# need to determine what languages to use in dataset\n",
    "# parse in languages BUT!! all values are words :( so use dict from previous\n",
    "\n",
    "\n",
    "# might change this datastructure later, but it works well with pandase\n",
    "data = {\"languages\": [], \"features\": [], \"values\": []}\n",
    "data_v2 = {}\n",
    "file_path = './wals_dataset.cldf/'\n",
    "with open(file_path + 'values.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        uuid, lang_id, param_id, val, code_id, comment, source, contrib_id = row\n",
    "        data['languages'].append(lang_id)\n",
    "        data['features'].append(param_id)\n",
    "        data['values'].append(param_info[param_id][val])  # value of variable\n",
    "        if param_id not in data_v2.keys():\n",
    "            data_v2[param_id] = dict()\n",
    "        data_v2[param_id][lang_id] = param_info[param_id][val]\n",
    "\n",
    "df_2 = pd.DataFrame.from_dict(data_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take top 40 categories intersection of languages & count\n",
    "counts = {}\n",
    "for param in data_v2.keys():\n",
    "    counts[param] = 0\n",
    "    for lang in data_v2[param]:\n",
    "        counts[param] += 1\n",
    "\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# remove features with <200 languages? <100 languages?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:remove sparse data\n",
    "\n",
    "\n",
    "# now count # of languages in all of them\n",
    "#print(data_v2.keys())\n",
    "\n",
    "\n",
    "def get_set(lt_num=1000):\n",
    "    lang_set = set(data_v2['1A']).intersection(set(data_v2['13A']))\n",
    "    param_set = []\n",
    "    for x, y in sorted_counts:\n",
    "        if lang_set is None:\n",
    "            lang_set = set(data_v2[x].keys())\n",
    "        else:\n",
    "            lang_set_post = lang_set.intersection(set(data_v2[x].keys()))\n",
    "            if len(lang_set_post) < lt_num:\n",
    "                continue\n",
    "            else:\n",
    "                lang_set = lang_set_post\n",
    "                param_set.append(x)\n",
    "    return lang_set, param_set\n",
    "\n",
    "\n",
    "def get_set_better(lt_num=1000, nec_set=['1A', '13A','26A']):\n",
    "    lang_set = set(data_v2[nec_set[0]].keys())\n",
    "    param_set = set(data_v2.keys())\n",
    "    params_out=set()\n",
    "    for feat in nec_set:\n",
    "        lang_set = lang_set.intersection(set(data_v2[feat].keys()))\n",
    "        param_set.remove(feat)\n",
    "        params_out.add(feat)\n",
    "    while len(lang_set) > lt_num:\n",
    "        best_set = {}\n",
    "        best_feat = {}\n",
    "        for feat in param_set:\n",
    "            cur_set = lang_set.intersection((set(data_v2[feat].keys())))\n",
    "            if len(cur_set) > len(best_set):\n",
    "                best_set = cur_set\n",
    "                best_feat = feat\n",
    "        param_set.remove(best_feat)\n",
    "        params_out.add(best_feat)\n",
    "        lang_set = best_set\n",
    "    return lang_set, params_out\n",
    "\n",
    "\n",
    "'''\n",
    "print(len(get_set(1000)[1]))\n",
    "print(len(get_set(700)[1]))\n",
    "print(len(get_set(500)[1]))#15 parameters for 500 languages\n",
    "print(len(get_set(200)[1]))#25 parameters for 200\n",
    "print(len(get_set(lt_num=100)[1]))#30 parameters for 100 languages?\n",
    "for i in range(1000,100,-10):\n",
    "    print(i,len(get_set_better(i,['26A','1A','13A'])[1]))\n",
    "'''\n",
    "\n",
    "# 220 languages with 25 parameters seems like a good equlibirium. Let's look at the languages that are there\n",
    "#but 200 with 31 parameters and the one's I want is better\n",
    "\n",
    "langs, params = get_set_better(200)\n",
    "\n",
    "# print(langs)\n",
    "\n",
    "\n",
    "# need to import our language metadata to understand where these languages are from to get an idea of the distribution\n",
    "\n",
    "\n",
    "# languages.csv :ID,Name,Macroarea,Latitude,Longitude,Glottocode,ISO639P3code\n",
    "lang_to_names = dict()\n",
    "with open(file_path + 'languages.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        lang_id, name, macro_area, lat, long, glottocode, iso_code = row\n",
    "        lang_to_names[lang_id] = name\n",
    "\n",
    "# looks like a prety good mix of langauges (although mainly euro focused) We can change up the dataset later\n",
    "'''\n",
    "\n",
    "for lang in langs:\n",
    "    print(lang_to_names[lang])\n",
    "'''\n",
    "\n",
    "# let's also take a look at what features we get from having 220 languages selected\n",
    "\n",
    "\n",
    "# parameters.csv : ID,Name,Description\n",
    "\n",
    "param_id_name = dict()\n",
    "with open(file_path + 'parameters.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        param_id, param_name, descr = row\n",
    "        param_id_name[param_id] = param_name\n",
    "\n",
    "'''\n",
    "\n",
    "for param in params:\n",
    "    print(param_id_name[param])\n",
    "# most of these parameters are word order and morphological\n",
    "'''\n",
    "\n",
    "fin_dataset = dict()\n",
    "for param in params:\n",
    "    fin_dataset[param] = dict()\n",
    "    for lang in langs:\n",
    "        fin_dataset[param][lang] = data_v2[param][lang]\n",
    "\n",
    "final_dataset= pd.DataFrame.from_dict(fin_dataset)\n",
    "#print(final_dataset)\n",
    "final_dataset.to_csv(\"dataset.csv\")\n",
    "\n",
    "\n",
    "#get dataset with top 100 features\n",
    "def get_set_incl(n_feat=140):\n",
    "    lang_set = set(data_v2['1A']).intersection(set(data_v2['13A']))\n",
    "    param_set = []\n",
    "    for x, y in sorted_counts[:n_feat]:\n",
    "        if lang_set is None:\n",
    "            lang_set = set(data_v2[x].keys())\n",
    "        else:\n",
    "            lang_set = lang_set.union(set(data_v2[x].keys()))\n",
    "            param_set.append(x)\n",
    "    inc_dataset = dict()\n",
    "    for param in param_set:\n",
    "        inc_dataset[param] = dict()\n",
    "    for lang in lang_set:\n",
    "        nan_count = 0\n",
    "        out_vec = dict()\n",
    "        for param in param_set:\n",
    "            try:\n",
    "                out_vec[param]=(data_v2[param][lang])\n",
    "                #inc_dataset[param][lang] = data_v2[param][lang]\n",
    "            except:\n",
    "                nan_count+=1\n",
    "                out_vec[param]=(np.nan)\n",
    "        if nan_count<40:\n",
    "            for param in param_set:\n",
    "                inc_dataset[param][lang] = out_vec[param]\n",
    "\n",
    "    return pd.DataFrame.from_dict(inc_dataset)\n",
    "\n",
    "incl_dataset= get_set_incl()\n",
    "print(incl_dataset)\n",
    "incl_dataset.to_csv('inc_dataset.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
