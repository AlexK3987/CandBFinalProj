{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from kmodes.kmodes import KModes\n",
    "import csv\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "# codes.csv : ID,Parameter_ID,Name,Description,Number\n",
    "# languages.csv :ID,Name,Macroarea,Latitude,Longitude,Glottocode,ISO639P3code\n",
    "# parameters.csv : ID,Name,Description\n",
    "# values.csv : ID,Language_ID,Parameter_ID,Value,Code_ID,Comment,Source,Contribution_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to determine if category is binary, categorical, or continuous`\n",
    "# need a map from (param_id,name) -> number\n",
    "param_info = dict()\n",
    "file_path = './wals_dataset.cldf/'\n",
    "with open(file_path + 'codes.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        uuid, param_id, name, desc, num = row\n",
    "        if param_id not in param_info.keys():\n",
    "            param_info[param_id] = dict()\n",
    "        param_info[param_id][name] = int(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Determine categorical variables\n",
    "\n",
    "# need to determine what languages to use in dataset\n",
    "# parse in languages BUT!! all values are words :( so use dict from previous\n",
    "\n",
    "\n",
    "# might change this datastructure later, but it works well with pandase\n",
    "data = {\"languages\": [], \"features\": [], \"values\": []}\n",
    "data_v2 = {}\n",
    "file_path = './wals_dataset.cldf/'\n",
    "with open(file_path + 'values.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        uuid, lang_id, param_id, val, code_id, comment, source, contrib_id = row\n",
    "        data['languages'].append(lang_id)\n",
    "        data['features'].append(param_id)\n",
    "        data['values'].append(param_info[param_id][val])  # value of variable\n",
    "        if param_id not in data_v2.keys():\n",
    "            data_v2[param_id] = dict()\n",
    "        data_v2[param_id][lang_id] = param_info[param_id][val]\n",
    "\n",
    "df_2 = pd.DataFrame.from_dict(data_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take top 40 categories intersection of languages & count\n",
    "counts = {}\n",
    "for param in data_v2.keys():\n",
    "    counts[param] = 0\n",
    "    for lang in data_v2[param]:\n",
    "        counts[param] += 1\n",
    "\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# remove features with <200 languages? <100 languages?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:remove sparse data\n",
    "\n",
    "\n",
    "# now count # of languages in all of them\n",
    "#print(data_v2.keys())\n",
    "\n",
    "\n",
    "def get_set(lt_num=1000):\n",
    "    lang_set = set(data_v2['1A']).intersection(set(data_v2['13A']))\n",
    "    param_set = []\n",
    "    for x, y in sorted_counts:\n",
    "        if lang_set is None:\n",
    "            lang_set = set(data_v2[x].keys())\n",
    "        else:\n",
    "            lang_set_post = lang_set.intersection(set(data_v2[x].keys()))\n",
    "            if len(lang_set_post) < lt_num:\n",
    "                continue\n",
    "            else:\n",
    "                lang_set = lang_set_post\n",
    "                param_set.append(x)\n",
    "    return lang_set, param_set\n",
    "\n",
    "\n",
    "def get_set_better(lt_num=1000, nec_set=['1A', '13A','26A']):\n",
    "    lang_set = set(data_v2[nec_set[0]].keys())\n",
    "    param_set = set(data_v2.keys())\n",
    "    params_out=set()\n",
    "    for feat in nec_set:\n",
    "        lang_set = lang_set.intersection(set(data_v2[feat].keys()))\n",
    "        param_set.remove(feat)\n",
    "        params_out.add(feat)\n",
    "    while len(lang_set) > lt_num:\n",
    "        best_set = {}\n",
    "        best_feat = {}\n",
    "        for feat in param_set:\n",
    "            cur_set = lang_set.intersection((set(data_v2[feat].keys())))\n",
    "            if len(cur_set) > len(best_set):\n",
    "                best_set = cur_set\n",
    "                best_feat = feat\n",
    "        param_set.remove(best_feat)\n",
    "        params_out.add(best_feat)\n",
    "        lang_set = best_set\n",
    "    return lang_set, params_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(len(get_set(1000)[1]))\n",
    "print(len(get_set(700)[1]))\n",
    "print(len(get_set(500)[1]))#15 parameters for 500 languages\n",
    "print(len(get_set(200)[1]))#25 parameters for 200\n",
    "print(len(get_set(lt_num=100)[1]))#30 parameters for 100 languages?\n",
    "for i in range(1000,100,-10):\n",
    "    print(i,len(get_set_better(i,['26A','1A','13A'])[1]))\n",
    "'''\n",
    "\n",
    "# 220 languages with 25 parameters seems like a good equlibirium. Let's look at the languages that are there\n",
    "#but 200 with 31 parameters and the one's I want is better\n",
    "\n",
    "langs, params = get_set_better(200)\n",
    "\n",
    "# print(langs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83A  82A  81A  87A  143F  143E  143A  143G  97A  86A  ...  29A  131A  \\\n",
      "ckr    1    1  1.0  2.0   1.0   4.0   2.0   4.0  2.0  1.0  ...    2   NaN   \n",
      "arp    2    1  2.0  1.0   1.0   1.0  15.0   4.0  3.0  3.0  ...    3   NaN   \n",
      "bsq    1    1  1.0  2.0   4.0   1.0   1.0   4.0  2.0  1.0  ...    3   2.0   \n",
      "war    2    2  4.0  NaN   4.0   1.0   1.0   4.0  NaN  2.0  ...    1   6.0   \n",
      "kse    1    1  1.0  2.0   4.0   1.0   1.0   4.0  2.0  1.0  ...    1   1.0   \n",
      "..   ...  ...  ...  ...   ...   ...   ...   ...  ...  ...  ...  ...   ...   \n",
      "awp    1    1  1.0  1.0   3.0   1.0  14.0   4.0  1.0  1.0  ...    2   6.0   \n",
      "kho    1    1  1.0  1.0   1.0   4.0   2.0   4.0  1.0  1.0  ...    2   1.0   \n",
      "hai    1    1  1.0  2.0   2.0   1.0  14.0   4.0  2.0  1.0  ...    1   1.0   \n",
      "abk    1    1  1.0  2.0   2.0   2.0  11.0   4.0  2.0  1.0  ...    3   2.0   \n",
      "ind    2    1  2.0  2.0   4.0   1.0   1.0   4.0  4.0  2.0  ...    1   1.0   \n",
      "\n",
      "     108A  79A  80A  79B  90B  98A  108B  109A  \n",
      "ckr   2.0  4.0  3.0  5.0  NaN  1.0   NaN   8.0  \n",
      "arp   3.0  4.0  1.0  5.0  NaN  1.0   4.0   3.0  \n",
      "bsq   2.0  1.0  1.0  5.0  1.0  6.0   2.0   8.0  \n",
      "war   3.0  4.0  3.0  5.0  NaN  1.0   4.0   8.0  \n",
      "kse   1.0  4.0  1.0  2.0  NaN  1.0   1.0   5.0  \n",
      "..    ...  ...  ...  ...  ...  ...   ...   ...  \n",
      "awp   3.0  1.0  1.0  5.0  NaN  2.0   4.0   8.0  \n",
      "kho   3.0  1.0  1.0  5.0  1.0  2.0   4.0   3.0  \n",
      "hai   NaN  NaN  NaN  NaN  NaN  1.0   NaN   NaN  \n",
      "abk   3.0  1.0  1.0  5.0  1.0  1.0   4.0   4.0  \n",
      "ind   3.0  4.0  1.0  5.0  NaN  1.0   4.0   2.0  \n",
      "\n",
      "[129 rows x 140 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# need to import our language metadata to understand where these languages are from to get an idea of the distribution\n",
    "\n",
    "\n",
    "# languages.csv :ID,Name,Macroarea,Latitude,Longitude,Glottocode,ISO639P3code\n",
    "lang_to_names = dict()\n",
    "with open(file_path + 'languages.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        lang_id, name, macro_area, lat, long, glottocode, iso_code = row\n",
    "        lang_to_names[lang_id] = name\n",
    "\n",
    "# looks like a prety good mix of langauges (although mainly euro focused) We can change up the dataset later\n",
    "'''\n",
    "\n",
    "for lang in langs:\n",
    "    print(lang_to_names[lang])\n",
    "'''\n",
    "\n",
    "# let's also take a look at what features we get from having 220 languages selected\n",
    "\n",
    "\n",
    "# parameters.csv : ID,Name,Description\n",
    "\n",
    "param_id_name = dict()\n",
    "with open(file_path + 'parameters.csv', 'r') as codes:\n",
    "    file_reader = csv.reader(codes, delimiter=',')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        param_id, param_name, descr = row\n",
    "        param_id_name[param_id] = param_name\n",
    "\n",
    "'''\n",
    "\n",
    "for param in params:\n",
    "    print(param_id_name[param])\n",
    "# most of these parameters are word order and morphological\n",
    "'''\n",
    "\n",
    "fin_dataset = dict()\n",
    "for param in params:\n",
    "    fin_dataset[param] = dict()\n",
    "    for lang in langs:\n",
    "        fin_dataset[param][lang] = data_v2[param][lang]\n",
    "\n",
    "final_dataset= pd.DataFrame.from_dict(fin_dataset)\n",
    "#print(final_dataset)\n",
    "final_dataset.to_csv(\"dataset.csv\")\n",
    "\n",
    "\n",
    "#get dataset with top 100 features\n",
    "def get_set_incl(n_feat=140):\n",
    "    lang_set = set(data_v2['1A']).intersection(set(data_v2['13A']))\n",
    "    param_set = []\n",
    "    for x, y in sorted_counts[:n_feat]:\n",
    "        if lang_set is None:\n",
    "            lang_set = set(data_v2[x].keys())\n",
    "        else:\n",
    "            lang_set = lang_set.union(set(data_v2[x].keys()))\n",
    "            param_set.append(x)\n",
    "    inc_dataset = dict()\n",
    "    for param in param_set:\n",
    "        inc_dataset[param] = dict()\n",
    "    for lang in lang_set:\n",
    "        nan_count = 0\n",
    "        out_vec = dict()\n",
    "        for param in param_set:\n",
    "            try:\n",
    "                out_vec[param]=(data_v2[param][lang])\n",
    "                #inc_dataset[param][lang] = data_v2[param][lang]\n",
    "            except:\n",
    "                nan_count+=1\n",
    "                out_vec[param]=(np.nan)\n",
    "        if nan_count<40:\n",
    "            for param in param_set:\n",
    "                inc_dataset[param][lang] = out_vec[param]\n",
    "\n",
    "    return pd.DataFrame.from_dict(inc_dataset)\n",
    "\n",
    "incl_dataset= get_set_incl()\n",
    "print(incl_dataset)\n",
    "incl_dataset.to_csv('inc_dataset.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_ord': 35, 'simpl_synt': 21, 'verb_cats': 17, 'lexicon': 34, 'morphology': 8, 'phonology': 19, 'nom_synt': 5, 'comp_sent': 1}\n"
     ]
    }
   ],
   "source": [
    "#add category of type of feature to datasets\n",
    "\n",
    "#incl_dataset\n",
    "#final_dataset\n",
    "\n",
    "#import cats\n",
    "import os\n",
    "import csv\n",
    "vals_to_type=dict()\n",
    "for filename in os.listdir(os.getcwd()+'/feats/'):\n",
    "    tp=filename.split(\".\")[0]\n",
    "    try:\n",
    "        fp= open(\"./feats/\"+filename,\"r\")\n",
    "    \n",
    "        file_reader = csv.reader(fp, delimiter=',')\n",
    "        next(file_reader)\n",
    "        for row in file_reader:\n",
    "            uuid=row[3]\n",
    "            vals_to_type[uuid]=tp\n",
    "        fp.close()\n",
    "    except:\n",
    "        continue\n",
    "tp_to_count = dict()\n",
    "for colname,_ in incl_dataset.iteritems():\n",
    "    try:\n",
    "        tp = vals_to_type[colname]\n",
    "        try:\n",
    "            tp_to_count[tp]+=1\n",
    "        except:\n",
    "            tp_to_count[tp]=1\n",
    "    except:\n",
    "        print(colname)\n",
    "print(tp_to_count)\n",
    "data = incl_dataset.copy()\n",
    "for colname,_ in incl_dataset.iteritems():\n",
    "    if vals_to_type[colname] == \"lexicon\" or vals_to_type[colname] == \"phonology\" or vals_to_type[colname] == \"morphology\":\n",
    "        del data[colname]\n",
    "data.to_csv(\"./var_ds/syntax.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
